{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, Dense, Lambda, Multiply, Subtract, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Activation, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Text preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Misc.\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pretty_midi\n",
    "\n",
    "SEED = 42\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/liavba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/liavba/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/liavba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_midis_paths(df):\n",
    "    midis = list(os.listdir(r'./datasets/midi_files'))\n",
    "    midis = {midi.lower()[:-4]: midi for midi in midis}    \n",
    "    \n",
    "    def combine_singer_song(singer, song):\n",
    "        key = f'{singer} - {song}'.replace(' ', '_').lower()\n",
    "        return midis[key] if key in midis else None\n",
    "    \n",
    "    df['midi_path'] = df.apply(lambda r: combine_singer_song(r['Singer'], r['Song Name']) ,axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Singer', 'Song Name', 'Lyrics']\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/lyrics_train_set.csv', names=cols)\n",
    "df = add_midis_paths(df)\n",
    "df_test = pd.read_csv('datasets/lyrics_test_set.csv', names=cols)\n",
    "df_test = add_midis_paths(df_test)\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "df_train = df[msk]\n",
    "df_val = df[~msk]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7fd936f3cb90>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm = pretty_midi.PrettyMIDI('datasets/midi_files/aladdin_-_A_whole_new_world.mid')\n",
    "pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 time signature changes\n",
      "There are 9 instruments\n",
      "Instrument 3 has 227 notes\n",
      "Instrument 4 has 0 pitch bends\n",
      "Instrument 5 has 0 control changes\n"
     ]
    }
   ],
   "source": [
    "print('There are {} time signature changes'.format(len(pm.time_signature_changes)))\n",
    "print('There are {} instruments'.format(len(pm.instruments)))\n",
    "print('Instrument 3 has {} notes'.format(len(pm.instruments[0].notes)))\n",
    "print('Instrument 4 has {} pitch bends'.format(len(pm.instruments[4].pitch_bends)))\n",
    "print('Instrument 5 has {} control changes'.format(len(pm.instruments[5].control_changes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye norma jean & though i never knew you at all & you had the grace to hold yourself & while those around you crawled & they crawled out of the woodwork & and they whispered into your brain & they set you on the treadmill & and they made you change your name & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & loneliness was tough & the toughest role you ever played & hollywood created a superstar & and pain was the price you paid & even when you died & oh the press still hounded you & all the papers had to say & was that marilyn was found in the nude & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & goodbye norma jean & though i never knew you at all & you had the grace to hold yourself & while those around you crawled & goodbye norma jean & from the young man in the twenty second row & who sees you as something as more than sexual & more than just our marilyn monroe & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & the candle burned out long before & your legend ever did &'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = df_train.iloc[0, 2]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"in\\'\", \"ing\", phrase)\n",
    "    phrase = re.sub(r\"y\\'all\", \"you all\", phrase)\n",
    "    \n",
    "    # punctions\n",
    "    regex = re.compile('[^a-zA-Z& ]')\n",
    "    phrase = regex.sub('', phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "def preprocess_lyrics(data):\n",
    "    data = decontracted(data)\n",
    "    tokens = word_tokenize(data)\n",
    "    data_arr = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        # Use only words, character combinations and numbers \n",
    "#         if not t.isalpha(): \n",
    "#             continue\n",
    "            \n",
    "        # Lower case word\n",
    "        t = t.lower()\n",
    "        \n",
    "#         # Remove stop words\n",
    "#         if t in sw: \n",
    "#             continue\n",
    "        \n",
    "        data_arr.append(t)\n",
    "    \n",
    "    \n",
    "    return data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's not time to make a change   & just relax take it easy   & you're still young that's your fault   & there's so much you have to know   & find a girl settle down   & if you want you can marry   & look at me i am old but i'm happy   &    & i was once like you are now   & and i know that it's not easy   & to be calm when you've found   & something going on   & but take your time think a lot   & think of everything you've got   & for you will still be here tomorrow   & but your dreams may not   &    & how can i try to explain?   & when i do he turns away again   & it's always been the same same old story   & from the moment i could talk   & i was ordered to listen   & now there's a way   & and i know that i have to go away   & i know i have to go   &    & it's not time to make a change   & just sit down take it slowly   & you're still young that's your fault   & there's so much you have to go through   & find a girl settle down   & if you want you can marry   & look at me i am old but i'm happy   &    & all the times that i've cried   & keeping all the things i knew inside   & it's hard but it's harder to ignore it   & if they were right i'd agree   & but it's them they know not me   & now there's a way   & and i know that i have to go away   & i know i have to go &\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[8,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is not time to make a change \n",
      "\n",
      "just relax take it easy \n",
      "\n",
      "you are still young that is your fault \n",
      "\n",
      "there is so much you have to know \n",
      "\n",
      "find a girl settle down \n",
      "\n",
      "if you want you can marry \n",
      "\n",
      "look at me i am old but i am happy \n",
      "\n",
      "\n",
      "\n",
      "i was once like you are now \n",
      "\n",
      "and i know that it is not easy \n",
      "\n",
      "to be calm when you have found \n",
      "\n",
      "something going on \n",
      "\n",
      "but take your time think a lot \n",
      "\n",
      "think of everything you have got \n",
      "\n",
      "for you will still be here tomorrow \n",
      "\n",
      "but your dreams may not \n",
      "\n",
      "\n",
      "\n",
      "how can i try to explain \n",
      "\n",
      "when i do he turns away again \n",
      "\n",
      "it is always been the same same old story \n",
      "\n",
      "from the moment i could talk \n",
      "\n",
      "i was ordered to listen \n",
      "\n",
      "now there is a way \n",
      "\n",
      "and i know that i have to go away \n",
      "\n",
      "i know i have to go \n",
      "\n",
      "\n",
      "\n",
      "it is not time to make a change \n",
      "\n",
      "just sit down take it slowly \n",
      "\n",
      "you are still young that is your fault \n",
      "\n",
      "there is so much you have to go through \n",
      "\n",
      "find a girl settle down \n",
      "\n",
      "if you want you can marry \n",
      "\n",
      "look at me i am old but i am happy \n",
      "\n",
      "\n",
      "\n",
      "all the times that i have cried \n",
      "\n",
      "keeping all the things i knew inside \n",
      "\n",
      "it is hard but it is harder to ignore it \n",
      "\n",
      "if they were right i would agree \n",
      "\n",
      "but it is them they know not me \n",
      "\n",
      "now there is a way \n",
      "\n",
      "and i know that i have to go away \n",
      "\n",
      "i know i have to go \n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = df_train.iloc[8,2]\n",
    "tokenized_string = preprocess_lyrics(string)\n",
    "\n",
    "def pretty_lyrics(tokenized_string):\n",
    "    for token in tokenized_string:\n",
    "        if token == '&':\n",
    "            print('\\n')\n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "\n",
    "pretty_lyrics(tokenized_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3      [come, on, check, it, out, ya, will, &, come, ...\n",
       "4      [let, the, beat, control, your, body, &, let, ...\n",
       "7      [now, that, i, have, lost, everything, to, you...\n",
       "11     [now, i, have, been, happy, lately, &, thinkin...\n",
       "24     [woohoo, &, woohoo, &, woohoo, &, woohoo, &, i...\n",
       "                             ...                        \n",
       "596    [hi, my, name, is, what, &, my, name, is, who,...\n",
       "597    [whatever, &, dre, just, let, it, run, &, ey, ...\n",
       "600    [okay, &, i, am, going, to, attempt, to, drown...\n",
       "603    [meet, eddie, twentythree, years, old, &, fed,...\n",
       "614    [you, all, know, me, still, the, same, og, but...\n",
       "Name: Lyrics, Length: 113, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_train = df_train['Lyrics'].apply(lambda s: preprocess_lyrics(s)[:-1] + ['$'])\n",
    "lyrics_val = df_val['Lyrics'].apply(lambda s: preprocess_lyrics(s)[:-1] + ['$'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lyrics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train = tokenizer.texts_to_sequences(lyrics_train)\n",
    "lyrics_val = tokenizer.texts_to_sequences(lyrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "if not os.path.isfile(EMBEDDING_FILE):\n",
    "    !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "    !gzip -f -d GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541 out of 6377 has no embedings from word2vec\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "embeddings_index = models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "embed_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index) + 1\n",
    "\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = (np.random.rand(nb_words+1, embed_size) - 0.5) / 5.0\n",
    "\n",
    "not_in_word2vec = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        not_in_word2vec += 1\n",
    "        \n",
    "print(f'{not_in_word2vec} out of {len(word_index)} has no embedings from word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying one word to whole song but one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((156330, 1204), (156330, 6670))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_x, train_y = [], []\n",
    "\n",
    "# for lyric in lyrics:\n",
    "#     for i in range(1, len(lyric)):\n",
    "#         train_x.append(lyric[:i])\n",
    "#         train_y.append(*lyric[i:i+1])\n",
    "        \n",
    "# train_x = pad_sequences(train_x)\n",
    "# train_y = to_categorical(train_y)\n",
    "# train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying sliding window of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast=np.lib.index_tricks.as_strided\n",
    "def generate_sliding_window(arr, window_size=5, window_stride=1, last_window=False):\n",
    "    last_window = 1 if last_window else 0\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "    arr_len = arr.shape[0]\n",
    "    s, = arr.strides\n",
    "    windows_num = ((arr_len-window_size)//window_stride) + last_window\n",
    "    \n",
    "    return ast(arr, (windows_num, window_size), (s*window_stride, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# window_size = 10\n",
    "def split_x_y(lyrics, window_size=13):\n",
    "    X, y = [], []\n",
    "    for lyric in lyrics:\n",
    "        X.append(generate_sliding_window(lyric, window_size))\n",
    "        y.append(lyric[window_size:])\n",
    "        \n",
    "    X = np.concatenate(X)\n",
    "    y = to_categorical(np.concatenate(y), num_classes=max_features)\n",
    "    \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = split_x_y(lyrics_train)\n",
    "val_data = split_x_y(lyrics_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_simple(seq_len):\n",
    "    inp = Input(shape=(seq_len,))\n",
    "    \n",
    "    embd = Embedding(max_features, \n",
    "                      embed_size, \n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=seq_len,\n",
    "                      name='word_embd')(inp)\n",
    "    \n",
    "    lstm = LSTM(100, return_sequences=True)(embd)\n",
    "    lstm = LSTM(100)(lstm)\n",
    "\n",
    "    X = Dense(100, activation=\"relu\")(lstm)\n",
    "    X = Dropout(0.5)(X)\n",
    "    out = Dense(max_features, activation=\"softmax\", name = 'out')(X)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "#     model.get_layer('embd').trainable = False\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    acc = 'val_loss'\n",
    "    acc_mode = 'min'\n",
    "#     acc = 'val_acc'\n",
    "#     acc_mode = 'max'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                              fr'./models/{model_name}.h5', \n",
    "                              monitor=acc, \n",
    "#                               verbose=1, \n",
    "                              save_best_only=True, \n",
    "                              mode=acc_mode)\n",
    "    earlystop = EarlyStopping(monitor=acc, mode=acc_mode, verbose=0, patience=6)\n",
    "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', patience = 5,\n",
    "                            factor = 0.5, min_lr = 1e-6, verbose = 0)\n",
    "\n",
    "    return [checkpoint, reduceLR] #earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_data, val_data, use_saved=False, params_dict=None):\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    params = ''\n",
    "    if params_dict is not None:\n",
    "        params = '_'.join(f'{key}_{val}' for key,val in params_dict.items())\n",
    "    model_name = 'simple_model' + f'_{params}'\n",
    "    \n",
    "    \n",
    "    if use_saved:\n",
    "        history = joblib.load(fr'./models/{model_name}_history.sav')\n",
    "    else:\n",
    "        callbacks = get_callbacks(model_name)\n",
    "        history = model.fit(\n",
    "                            x=train_x,\n",
    "                            y=train_y,\n",
    "                            batch_size=params_dict['batch_size'],\n",
    "                            epochs=params_dict['epochs'],\n",
    "                            validation_split=params_dict['validation_split'],\n",
    "                            callbacks=callbacks,\n",
    "                            verbose=1\n",
    "                            )\n",
    "        \n",
    "        history = history.history\n",
    "        joblib.dump(history, fr'./models/{model_name}_history.sav')\n",
    "    \n",
    "    model = load_model(fr'./models/{model_name}.h5')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_gen, train_data, val_data, use_saved=False, params_dict=None):\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    params = ''\n",
    "    if params_dict is not None:\n",
    "        params = '_'.join(f'{key}_{val}' for key,val in params_dict.items())\n",
    "    model_name = model_gen.__name__[5:] + f'_{params}'\n",
    "        \n",
    "    if use_saved:\n",
    "        history = joblib.load(fr'./models/{model_name}_history.sav')\n",
    "    else:\n",
    "        callbacks = get_callbacks(model_name)\n",
    "        \n",
    "        train_x, train_y = train_data\n",
    "        \n",
    "        model = model_gen(train_x.shape[1]) # window size\n",
    "        history = model.fit(\n",
    "                            x=train_x,\n",
    "                            y=train_y,\n",
    "                            batch_size=params_dict['batch_size'],\n",
    "                            epochs=params_dict['epochs'],\n",
    "                            validation_data=val_data,\n",
    "                            callbacks=callbacks,\n",
    "                            verbose=1\n",
    "                            )\n",
    "        \n",
    "        history = history.history\n",
    "        joblib.dump(history, fr'./models/{model_name}_history.sav')\n",
    "    \n",
    "    model = load_model(fr'./models/{model_name}.h5')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 1204)]            0         \n",
      "_________________________________________________________________\n",
      "word_embd (Embedding)        (None, 1204, 300)         1913400   \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 1204, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 6378)              644178    \n",
      "=================================================================\n",
      "Total params: 2,808,478\n",
      "Trainable params: 2,808,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = init_simple(train_x.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4619/4619 [==============================] - 86s 17ms/step - loss: 5.7913 - val_loss: 5.0301\n",
      "Epoch 2/20\n",
      "4619/4619 [==============================] - 70s 15ms/step - loss: 4.9867 - val_loss: 4.8607\n",
      "Epoch 3/20\n",
      "4619/4619 [==============================] - 69s 15ms/step - loss: 4.6565 - val_loss: 4.8774\n",
      "Epoch 4/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 4.4195 - val_loss: 4.9408\n",
      "Epoch 5/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 4.2456 - val_loss: 5.0937\n",
      "Epoch 6/20\n",
      "4619/4619 [==============================] - 77s 17ms/step - loss: 4.0573 - val_loss: 5.2618\n",
      "Epoch 7/20\n",
      "4619/4619 [==============================] - 76s 17ms/step - loss: 3.9183 - val_loss: 5.4201\n",
      "Epoch 8/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.7498 - val_loss: 5.7651\n",
      "Epoch 9/20\n",
      "4619/4619 [==============================] - 75s 16ms/step - loss: 3.6126 - val_loss: 5.9113\n",
      "Epoch 10/20\n",
      "4619/4619 [==============================] - 77s 17ms/step - loss: 3.5423 - val_loss: 6.2786\n",
      "Epoch 11/20\n",
      "4619/4619 [==============================] - 77s 17ms/step - loss: 3.4657 - val_loss: 6.6475\n",
      "Epoch 12/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.3858 - val_loss: 6.7223\n",
      "Epoch 13/20\n",
      "4619/4619 [==============================] - 76s 17ms/step - loss: 3.2913 - val_loss: 7.0218\n",
      "Epoch 14/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.2235 - val_loss: 7.2949\n",
      "Epoch 15/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.1806 - val_loss: 7.3991\n",
      "Epoch 16/20\n",
      "4619/4619 [==============================] - 75s 16ms/step - loss: 3.1418 - val_loss: 7.8695\n",
      "Epoch 17/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.1013 - val_loss: 7.8988\n",
      "Epoch 18/20\n",
      "4619/4619 [==============================] - 76s 17ms/step - loss: 3.0494 - val_loss: 8.1687\n",
      "Epoch 19/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.0154 - val_loss: 8.3088\n",
      "Epoch 20/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 2.9887 - val_loss: 8.4389\n"
     ]
    }
   ],
   "source": [
    "params_dict = {'batch_size': 32, 'epochs': 20}\n",
    "model, history= train_model(init_simple, train_data, val_data, use_saved=F, params_dict=params_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147787, 6378)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def generate_song(model, seed, window_size, stop_token, tokenizer, max_len):\n",
    "    stop_token = tokenizer.word_index[stop_token]\n",
    "    \n",
    "    \n",
    "    def get_next_word(seed):\n",
    "        probs = model.predict(seed)\n",
    "        chosen_idx = np.random.choice(range(0, max_features), p=probs[0])\n",
    "        chosen_word = tokenizer.sequences_to_texts([[chosen_idx]])[0]\n",
    "        \n",
    "        return chosen_idx, chosen_word\n",
    "    \n",
    "    \n",
    "    seed = preprocess_lyrics(seed)\n",
    "    song = seed.copy()\n",
    "    seed = \" \".join(seed)\n",
    "    seed = tokenizer.texts_to_sequences([seed])\n",
    "    seed = pad_sequences(seed, maxlen=window_size)\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    idx, word = get_next_word(seed)\n",
    "    \n",
    "    \n",
    "    while word != stop_token and i < max_len:\n",
    "        song.append(word)\n",
    "        i+=1\n",
    "        seed = np.concatenate([seed[:,1:], [[idx]]], axis=1)\n",
    "        idx, word = get_next_word(seed)\n",
    "    \n",
    "    return song    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "song = generate_song(model, 'Smack that all on the floor', 10, '$', tokenizer, 1000)\n",
    "pretty_lyrics(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liav_notebook",
   "language": "python",
   "name": "liav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
