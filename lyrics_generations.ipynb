{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, Dense, Lambda, Multiply, Subtract, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Activation, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Misc.\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pretty_midi\n",
    "\n",
    "SEED = 42\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Singer', 'Song Name', 'Lyrics']\n",
    "\n",
    "df = pd.read_csv('datasets/lyrics_train_set.csv', names=cols)\n",
    "df_test = pd.read_csv('datasets/lyrics_test_set.csv', names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add midi files' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_midi_files(df):\n",
    "    midis = list(os.listdir(r'./datasets/midi_files'))\n",
    "    midis = {midi.lower()[:-4]: midi for midi in midis}    \n",
    "    \n",
    "    def combine_singer_song(singer, song):\n",
    "        key = f'{singer} - {song}'.replace(' ', '_').lower()\n",
    "        return midis[key] if key in midis else None\n",
    "    \n",
    "    df['Midi File'] = df.apply(lambda r: combine_singer_song(r['Singer'], r['Song Name']) ,axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_midi_files(df)\n",
    "df_test = add_midi_files(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check midi files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midis = np.concatenate([df['Midi File'].values, df_test['Midi File'].values])\n",
    "midis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not decode key with 16 sharps and mode 1\n",
      "error readying midi file David_Bowie_-_Lazarus.mid\n",
      "Could not decode key with 1 flats and mode 255\n",
      "error readying midi file Beastie_Boys_-_Girls.mid\n",
      "data byte must be in range 0..127\n",
      "error readying midi file Billy_Joel_-_Movin'_Out.mid\n",
      "data byte must be in range 0..127\n",
      "error readying midi file Billy_Joel_-_Pressure.mid\n",
      "Could not decode key with 4 flats and mode 255\n",
      "error readying midi file Dan_Fogelberg_-_Leader_of_the_Band.mid\n",
      "\n",
      "error readying midi file Brian_McKnight_-_On_The_Down_Low.mid\n",
      "data byte must be in range 0..127\n",
      "error readying midi file Aaron_Neville_-_Tell_It_Like_It_Is.mid\n"
     ]
    }
   ],
   "source": [
    "corrupted = []\n",
    "for i, midi in enumerate(midis):\n",
    "    try:\n",
    "        midi = pretty_midi.PrettyMIDI(fr'./datasets/midi_files/{midi}')\n",
    "        midi.remove_invalid_notes()\n",
    "        del midi\n",
    "    except Exception as e:\n",
    "        print(\"%s\\nerror readying midi file %s\" % (e, midi))\n",
    "        corrupted.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folowing files index are corrupted: [91, 115, 136, 143, 189, 513, 575]\n"
     ]
    }
   ],
   "source": [
    "print(f'The folowing files index are corrupted: {corrupted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Singer</th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Midi File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>david bowie</td>\n",
       "      <td>lazarus</td>\n",
       "      <td>look up here i'm in heaven &amp; i've got scars th...</td>\n",
       "      <td>David_Bowie_-_Lazarus.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>beastie boys</td>\n",
       "      <td>girls</td>\n",
       "      <td>girls all i really want is girls &amp; and in the ...</td>\n",
       "      <td>Beastie_Boys_-_Girls.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>billy joel</td>\n",
       "      <td>movin' out</td>\n",
       "      <td>anthony works in the grocery store   &amp; savin h...</td>\n",
       "      <td>Billy_Joel_-_Movin'_Out.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>billy joel</td>\n",
       "      <td>pressure</td>\n",
       "      <td>you have to learn to pace yourself &amp; pressure ...</td>\n",
       "      <td>Billy_Joel_-_Pressure.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>dan fogelberg</td>\n",
       "      <td>leader of the band</td>\n",
       "      <td>an only child alone and wild a cabinet maker's...</td>\n",
       "      <td>Dan_Fogelberg_-_Leader_of_the_Band.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>brian mcknight</td>\n",
       "      <td>on the down low</td>\n",
       "      <td>maxine was 5'9'' &amp; had a man and she didn't mi...</td>\n",
       "      <td>Brian_McKnight_-_On_The_Down_Low.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>aaron neville</td>\n",
       "      <td>tell it like it is</td>\n",
       "      <td>if you want something to play with &amp; go and fi...</td>\n",
       "      <td>Aaron_Neville_-_Tell_It_Like_It_Is.mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Singer           Song Name  \\\n",
       "91      david bowie             lazarus   \n",
       "115    beastie boys               girls   \n",
       "136      billy joel          movin' out   \n",
       "143      billy joel            pressure   \n",
       "189   dan fogelberg  leader of the band   \n",
       "513  brian mcknight     on the down low   \n",
       "575   aaron neville  tell it like it is   \n",
       "\n",
       "                                                Lyrics  \\\n",
       "91   look up here i'm in heaven & i've got scars th...   \n",
       "115  girls all i really want is girls & and in the ...   \n",
       "136  anthony works in the grocery store   & savin h...   \n",
       "143  you have to learn to pace yourself & pressure ...   \n",
       "189  an only child alone and wild a cabinet maker's...   \n",
       "513  maxine was 5'9'' & had a man and she didn't mi...   \n",
       "575  if you want something to play with & go and fi...   \n",
       "\n",
       "                                  Midi File  \n",
       "91                David_Bowie_-_Lazarus.mid  \n",
       "115                Beastie_Boys_-_Girls.mid  \n",
       "136             Billy_Joel_-_Movin'_Out.mid  \n",
       "143               Billy_Joel_-_Pressure.mid  \n",
       "189  Dan_Fogelberg_-_Leader_of_the_Band.mid  \n",
       "513    Brian_McKnight_-_On_The_Down_Low.mid  \n",
       "575  Aaron_Neville_-_Tell_It_Like_It_Is.mid  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[corrupted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.index.isin(corrupted)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "df_train = df[msk]\n",
    "df_val = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye norma jean & though i never knew you at all & you had the grace to hold yourself & while those around you crawled & they crawled out of the woodwork & and they whispered into your brain & they set you on the treadmill & and they made you change your name & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & loneliness was tough & the toughest role you ever played & hollywood created a superstar & and pain was the price you paid & even when you died & oh the press still hounded you & all the papers had to say & was that marilyn was found in the nude & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & goodbye norma jean & though i never knew you at all & you had the grace to hold yourself & while those around you crawled & goodbye norma jean & from the young man in the twenty second row & who sees you as something as more than sexual & more than just our marilyn monroe & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & the candle burned out long before & your legend ever did &'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = df_train.iloc[0, 2]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"in\\'\", \"ing\", phrase)\n",
    "    phrase = re.sub(r\"y\\'all\", \"you all\", phrase)\n",
    "    \n",
    "    # punctions\n",
    "    regex = re.compile('[^a-zA-Z& ]')\n",
    "    phrase = regex.sub('', phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "def preprocess_lyrics(data):\n",
    "    data = decontracted(data)\n",
    "    tokens = word_tokenize(data)\n",
    "    data_arr = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        # Use only words, character combinations and numbers \n",
    "#         if not t.isalpha(): \n",
    "#             continue\n",
    "            \n",
    "        # Lower case word\n",
    "        t = t.lower()\n",
    "        \n",
    "#         # Remove stop words\n",
    "#         if t in sw: \n",
    "#             continue\n",
    "        \n",
    "        data_arr.append(t)\n",
    "    \n",
    "    \n",
    "    return data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's not time to make a change   & just relax take it easy   & you're still young that's your fault   & there's so much you have to know   & find a girl settle down   & if you want you can marry   & look at me i am old but i'm happy   &    & i was once like you are now   & and i know that it's not easy   & to be calm when you've found   & something going on   & but take your time think a lot   & think of everything you've got   & for you will still be here tomorrow   & but your dreams may not   &    & how can i try to explain?   & when i do he turns away again   & it's always been the same same old story   & from the moment i could talk   & i was ordered to listen   & now there's a way   & and i know that i have to go away   & i know i have to go   &    & it's not time to make a change   & just sit down take it slowly   & you're still young that's your fault   & there's so much you have to go through   & find a girl settle down   & if you want you can marry   & look at me i am old but i'm happy   &    & all the times that i've cried   & keeping all the things i knew inside   & it's hard but it's harder to ignore it   & if they were right i'd agree   & but it's them they know not me   & now there's a way   & and i know that i have to go away   & i know i have to go &\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[8,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is not time to make a change \n",
      "\n",
      "just relax take it easy \n",
      "\n",
      "you are still young that is your fault \n",
      "\n",
      "there is so much you have to know \n",
      "\n",
      "find a girl settle down \n",
      "\n",
      "if you want you can marry \n",
      "\n",
      "look at me i am old but i am happy \n",
      "\n",
      "\n",
      "\n",
      "i was once like you are now \n",
      "\n",
      "and i know that it is not easy \n",
      "\n",
      "to be calm when you have found \n",
      "\n",
      "something going on \n",
      "\n",
      "but take your time think a lot \n",
      "\n",
      "think of everything you have got \n",
      "\n",
      "for you will still be here tomorrow \n",
      "\n",
      "but your dreams may not \n",
      "\n",
      "\n",
      "\n",
      "how can i try to explain \n",
      "\n",
      "when i do he turns away again \n",
      "\n",
      "it is always been the same same old story \n",
      "\n",
      "from the moment i could talk \n",
      "\n",
      "i was ordered to listen \n",
      "\n",
      "now there is a way \n",
      "\n",
      "and i know that i have to go away \n",
      "\n",
      "i know i have to go \n",
      "\n",
      "\n",
      "\n",
      "it is not time to make a change \n",
      "\n",
      "just sit down take it slowly \n",
      "\n",
      "you are still young that is your fault \n",
      "\n",
      "there is so much you have to go through \n",
      "\n",
      "find a girl settle down \n",
      "\n",
      "if you want you can marry \n",
      "\n",
      "look at me i am old but i am happy \n",
      "\n",
      "\n",
      "\n",
      "all the times that i have cried \n",
      "\n",
      "keeping all the things i knew inside \n",
      "\n",
      "it is hard but it is harder to ignore it \n",
      "\n",
      "if they were right i would agree \n",
      "\n",
      "but it is them they know not me \n",
      "\n",
      "now there is a way \n",
      "\n",
      "and i know that i have to go away \n",
      "\n",
      "i know i have to go \n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = df_train.iloc[8,2]\n",
    "tokenized_string = preprocess_lyrics(string)\n",
    "\n",
    "def pretty_lyrics(tokenized_string):\n",
    "    for token in tokenized_string:\n",
    "        if token == '&':\n",
    "            print('\\n')\n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "\n",
    "pretty_lyrics(tokenized_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3      [come, on, check, it, out, ya, will, &, come, ...\n",
       "4      [let, the, beat, control, your, body, &, let, ...\n",
       "7      [now, that, i, have, lost, everything, to, you...\n",
       "11     [now, i, have, been, happy, lately, &, thinkin...\n",
       "24     [woohoo, &, woohoo, &, woohoo, &, woohoo, &, i...\n",
       "                             ...                        \n",
       "596    [hi, my, name, is, what, &, my, name, is, who,...\n",
       "597    [whatever, &, dre, just, let, it, run, &, ey, ...\n",
       "600    [okay, &, i, am, going, to, attempt, to, drown...\n",
       "603    [meet, eddie, twentythree, years, old, &, fed,...\n",
       "614    [you, all, know, me, still, the, same, og, but...\n",
       "Name: Lyrics, Length: 113, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_token = '$'\n",
    "lyrics_train = df_train['Lyrics'].apply(lambda s: preprocess_lyrics(s)[:-1] + [stop_token])\n",
    "lyrics_val = df_val['Lyrics'].apply(lambda s: preprocess_lyrics(s)[:-1] + [stop_token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lyrics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train = tokenizer.texts_to_sequences(lyrics_train)\n",
    "lyrics_val = tokenizer.texts_to_sequences(lyrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "if not os.path.isfile(EMBEDDING_FILE):\n",
    "    !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "    !gzip -f -d GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541 out of 6377 has no embedings from word2vec\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "embeddings_index = models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "embed_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index) + 1\n",
    "\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = (np.random.rand(nb_words+1, embed_size) - 0.5) / 5.0\n",
    "\n",
    "not_in_word2vec = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        not_in_word2vec += 1\n",
    "        \n",
    "print(f'{not_in_word2vec} out of {len(word_index)} has no embedings from word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying one word to whole song but one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((156330, 1204), (156330, 6670))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_x, train_y = [], []\n",
    "\n",
    "# for lyric in lyrics:\n",
    "#     for i in range(1, len(lyric)):\n",
    "#         train_x.append(lyric[:i])\n",
    "#         train_y.append(*lyric[i:i+1])\n",
    "        \n",
    "# train_x = pad_sequences(train_x)\n",
    "# train_y = to_categorical(train_y)\n",
    "# train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying sliding window of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast=np.lib.index_tricks.as_strided\n",
    "def generate_sliding_window(arr, window_size=5, window_stride=1, last_window=False):\n",
    "    last_window = 1 if last_window else 0\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "    arr_len = arr.shape[0]\n",
    "    s, = arr.strides\n",
    "    windows_num = ((arr_len-window_size)//window_stride) + last_window\n",
    "    \n",
    "    return ast(arr, (windows_num, window_size), (s*window_stride, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_y(lyrics, window_size=10):\n",
    "    X, y = [], []\n",
    "    for lyric in lyrics:\n",
    "        X.append(generate_sliding_window(lyric, window_size))\n",
    "        y.append(lyric[window_size:])\n",
    "        \n",
    "    X = np.concatenate(X)\n",
    "    y = to_categorical(np.concatenate(y), num_classes=max_features)\n",
    "    \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 13\n",
    "train_data = split_x_y(lyrics_train, window_size)\n",
    "val_data = split_x_y(lyrics_val, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melody preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_melody_file(path, fs=5, max_piano_val=100, max_frames=795):\n",
    "    pm = pretty_midi.PrettyMIDI(path)\n",
    "    pm.remove_invalid_notes()\n",
    "    \n",
    "    # Sum all notes from all instruments\n",
    "    piano_all = pm.get_piano_roll(fs=fs)\n",
    "    piano_shape = piano_all.shape\n",
    "\n",
    "    # Normalize by the number of played instruments in the same time and note\n",
    "    counter = np.zeros(piano_shape)\n",
    "    for inst in pm.instruments:\n",
    "        curr_piano = inst.get_piano_roll(fs=fs)\n",
    "\n",
    "        counter[:, :curr_piano.shape[1]] += (curr_piano > 0).astype(int)\n",
    "\n",
    "    counter[counter == 0] = 1\n",
    "    piano_all /= counter\n",
    "    \n",
    "    # Normalize by the maximum value of a note\n",
    "    piano_all = piano_all / max_piano_val\n",
    "    \n",
    "    # Normalize by the number of played notes in the same time\n",
    "    count_notes = (piano_all > 0).sum(axis=0)\n",
    "    count_notes[count_notes == 0] = 1\n",
    "    piano_all = piano_all.sum(axis=0) / count_notes\n",
    "    \n",
    "    # Take maximum features from a song\n",
    "    piano_all = piano_all[:max_frames]\n",
    "    \n",
    "    return piano_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Lyrics Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_simple(seq_len):\n",
    "    inp = Input(shape=(seq_len,))\n",
    "    \n",
    "    embd = Embedding(max_features, \n",
    "                      embed_size, \n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=seq_len,\n",
    "                      name='word_embd')(inp)\n",
    "    \n",
    "    lstm = LSTM(100, return_sequences=True)(embd)\n",
    "    lstm = LSTM(100)(lstm)\n",
    "\n",
    "    X = Dense(100, activation=\"relu\")(lstm)\n",
    "    X = Dropout(0.5)(X)\n",
    "    out = Dense(max_features, activation=\"softmax\", name = 'out')(X)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "#     model.get_layer('embd').trainable = False\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    acc = 'val_loss'\n",
    "    acc_mode = 'min'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                              fr'./models/{model_name}.h5', \n",
    "                              monitor=acc, \n",
    "#                               verbose=1, \n",
    "                              save_best_only=True, \n",
    "                              mode=acc_mode)\n",
    "    earlystop = EarlyStopping(monitor=acc, mode=acc_mode, verbose=1, patience=6)\n",
    "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', patience = 5,\n",
    "                            factor = 0.5, min_lr = 1e-6, verbose = 1)\n",
    "\n",
    "    return [checkpoint, reduceLR] #earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_gen, train_data, val_data, use_saved=False, params_dict=None):\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    params = ''\n",
    "    if params_dict is not None:\n",
    "        params = '_'.join(f'{key}_{val}' for key,val in params_dict.items())\n",
    "    model_name = model_gen.__name__[5:] + f'_{params}'\n",
    "        \n",
    "    if use_saved:\n",
    "        history = joblib.load(fr'./models/{model_name}_history.sav')\n",
    "    else:\n",
    "        callbacks = get_callbacks(model_name)\n",
    "        \n",
    "        train_x, train_y = train_data\n",
    "        \n",
    "        model = model_gen(train_x.shape[1]) # window size\n",
    "        history = model.fit(\n",
    "                            x=train_x,\n",
    "                            y=train_y,\n",
    "                            batch_size=params_dict['batch_size'],\n",
    "                            epochs=params_dict['epochs'],\n",
    "                            validation_data=val_data,\n",
    "                            callbacks=callbacks,\n",
    "                            verbose=1\n",
    "                            )\n",
    "        \n",
    "        history = history.history\n",
    "        joblib.dump(history, fr'./models/{model_name}_history.sav')\n",
    "    \n",
    "    model = load_model(fr'./models/{model_name}.h5')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 1204)]            0         \n",
      "_________________________________________________________________\n",
      "word_embd (Embedding)        (None, 1204, 300)         1913400   \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 1204, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 6378)              644178    \n",
      "=================================================================\n",
      "Total params: 2,808,478\n",
      "Trainable params: 2,808,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = init_simple(train_x.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4619/4619 [==============================] - 86s 17ms/step - loss: 5.7913 - val_loss: 5.0301\n",
      "Epoch 2/20\n",
      "4619/4619 [==============================] - 70s 15ms/step - loss: 4.9867 - val_loss: 4.8607\n",
      "Epoch 3/20\n",
      "4619/4619 [==============================] - 69s 15ms/step - loss: 4.6565 - val_loss: 4.8774\n",
      "Epoch 4/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 4.4195 - val_loss: 4.9408\n",
      "Epoch 5/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 4.2456 - val_loss: 5.0937\n",
      "Epoch 6/20\n",
      "4619/4619 [==============================] - 77s 17ms/step - loss: 4.0573 - val_loss: 5.2618\n",
      "Epoch 7/20\n",
      "4619/4619 [==============================] - 76s 17ms/step - loss: 3.9183 - val_loss: 5.4201\n",
      "Epoch 8/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.7498 - val_loss: 5.7651\n",
      "Epoch 9/20\n",
      "4619/4619 [==============================] - 75s 16ms/step - loss: 3.6126 - val_loss: 5.9113\n",
      "Epoch 10/20\n",
      "4619/4619 [==============================] - 77s 17ms/step - loss: 3.5423 - val_loss: 6.2786\n",
      "Epoch 11/20\n",
      "4619/4619 [==============================] - 77s 17ms/step - loss: 3.4657 - val_loss: 6.6475\n",
      "Epoch 12/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.3858 - val_loss: 6.7223\n",
      "Epoch 13/20\n",
      "4619/4619 [==============================] - 76s 17ms/step - loss: 3.2913 - val_loss: 7.0218\n",
      "Epoch 14/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.2235 - val_loss: 7.2949\n",
      "Epoch 15/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.1806 - val_loss: 7.3991\n",
      "Epoch 16/20\n",
      "4619/4619 [==============================] - 75s 16ms/step - loss: 3.1418 - val_loss: 7.8695\n",
      "Epoch 17/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.1013 - val_loss: 7.8988\n",
      "Epoch 18/20\n",
      "4619/4619 [==============================] - 76s 17ms/step - loss: 3.0494 - val_loss: 8.1687\n",
      "Epoch 19/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 3.0154 - val_loss: 8.3088\n",
      "Epoch 20/20\n",
      "4619/4619 [==============================] - 76s 16ms/step - loss: 2.9887 - val_loss: 8.4389\n"
     ]
    }
   ],
   "source": [
    "params_dict = {'batch_size': 32, 'epochs': 20}\n",
    "model, history= train_model(init_simple, train_data, val_data, use_saved=F, params_dict=params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Lyrics by Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_song(model, seed, window_size, stop_token, tokenizer, max_len):\n",
    "    stop_token = tokenizer.word_index[stop_token]\n",
    "    \n",
    "    def get_next_word(seed):\n",
    "        probs = model.predict(seed)\n",
    "        chosen_idx = np.random.choice(range(0, max_features), p=probs[0])\n",
    "        chosen_word = tokenizer.sequences_to_texts([[chosen_idx]])[0]\n",
    "        \n",
    "        return chosen_idx, chosen_word\n",
    "    \n",
    "    \n",
    "    seed = preprocess_lyrics(seed)\n",
    "    song = seed.copy()\n",
    "    seed = ' '.join(seed)\n",
    "    seed = tokenizer.texts_to_sequences([seed])\n",
    "    seed = pad_sequences(seed, maxlen=window_size)\n",
    "\n",
    "    i = 0\n",
    "    idx, word = get_next_word(seed)\n",
    "    while word != stop_token and i < max_len:\n",
    "        song.append(word)\n",
    "        i+=1\n",
    "        seed = np.concatenate([seed[:,1:], [[idx]]], axis=1)\n",
    "        idx, word = get_next_word(seed)\n",
    "    \n",
    "    return song    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_max_length = 2000\n",
    "seed = 'Smack that all on the floor'\n",
    "song = generate_song(model, seed, window_size, stop_token, tokenizer, song_max_length)\n",
    "pretty_lyrics(song)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
